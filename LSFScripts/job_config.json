{ 
    "JobParams": {

        "CreateHash": {
            "outfile": "CreateHash_Job.q",
            "header": [
                "#SBATCH -J CreateHash",
                "#SBATCH -o {0}Logs/CreateHash-Out%I.out",
                "#SBATCH -e {0}Logs/CreateHash-Err%I.err"],
            "body": [
                "create_hash.py -i {0}original_reads/ -o {0}hashed_reads/ -k 33 -s 31"]},

        "HashReads": {
            "outfile": "HashReads_ArrayJob.q",
            "array": ["original_reads/","*.fastq.*"],
            "header": [
                "#SBATCH -J HashReads[1-{array_size}]",
                "#SBATCH --array=1-{array_size}",
                "#SBATCH -o {0}Logs/HashReads-Out%I.out",
                "#SBATCH -e {0}Logs/HashReads-Err%I.err",
                "#SBATCH -M 8"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))","hash_fastq_reads.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}original_reads/ -o {0}hashed_reads/"]},

        "MergeHash": {
            "outfile": "MergeHash_ArrayJob.q",
            "array": ["original_reads/","*.fastq",5],
            "header": [
                "#SBATCH -J MergeHash[1-{array_size}]",
                "#SBATCH --array=1-{array_size}",
                "#SBATCH -o {0}Logs/MergeHash-Out-%I.out",
                "#SBATCH -e {0}Logs/MergeHash-Err-%I.err",
                "#SBATCH -t 53:58",
                "#SBATCH -R 'rusage[mem=4]'",
                "#SBATCH -M 8"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "merge_hashq_files.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}hashed_reads/"]},

        "CombineFractions": {
            "outfile": "CombineFractions_ArrayJob.q",
            "array": ["original_reads/","*.fastq",1],
            "header": [
                "#SBATCH -J CombineFractions[1-{array_size}]",
                "#SBATCH --array=1-{array_size}",
                "#SBATCH -o {0}Logs/CombineFractions-Out-%I.out",
                "#SBATCH -e {0}Logs/CombineFractions-Err-%I.err",
                "#SBATCH -q week",
                "#SBATCH -t 23:58",
                "#SBATCH -R 'rusage[mem=8]'",
                "#SBATCH -M 20"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "merge_hashq_fractions.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}hashed_reads/"]},

        "GlobalWeights": {
            "outfile": "GlobalWeights_Job.q",
            "header": [
                "#SBATCH -J GlobalWeights",
                "#SBATCH -o {0}Logs/GlobalWeights-Out.out",
                "#SBATCH -e {0}Logs/GlobalWeights-Err.err",
                "#SBATCH -q week",
                "#SBATCH -t 71:10",
                "#SBATCH -R 'rusage[mem=25]'",
                "#SBATCH -M 75"],
            "body": [
                "tfidf_corpus.py -i {0}hashed_reads/ -o {0}cluster_vectors/"]},

        "KmerCorpus": {
            "outfile": "KmerCorpus_ArrayJob.q",
            "array": ["hashed_reads/","*.count.hash"],
            "header": [
                "#SBATCH -J KmerCorpus[1-{array_size}]",
                "#SBATCH --array=1-{array_size}",
                "#SBATCH -o {0}Logs/KmerCorpus-Out-%I.out",
                "#SBATCH -e {0}Logs/KmerCorpus-Err-%I.err",
                "#SBATCH -q hour",
                "#SBATCH -t 3:58",
                "#SBATCH -R 'rusage[mem=32]'",
                "#SBATCH -M 45"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "kmer_corpus.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}cluster_vectors/"]},

        "KmerLSI": {
            "outfile": "KmerLSI_Job.q",
            "header": [
                "#SBATCH -J KmerLSI",
                "#SBATCH -o {0}Logs/KmerLSI-Out.out",
                "#SBATCH -e {0}Logs/KmerLSI-Err.err",
                "#SBATCH -q week",
                "#SBATCH -n 6",
                "#SBATCH -R 'rusage[mem=4] span[hosts=1]'",
                "#SBATCH -M 10",
                "python -m Pyro4.naming -n 0.0.0.0 > {0}Logs/nameserver.log 2>&1 &","P1=$!","python -m gensim.models.lsi_worker > {0}Logs/worker1.log 2>&1 &","P2=$!",
                "python -m gensim.models.lsi_worker > {0}Logs/worker2.log 2>&1 &","P3=$!",
                "python -m gensim.models.lsi_worker > {0}Logs/worker3.log 2>&1 &","P4=$!",
                "python -m gensim.models.lsi_worker > {0}Logs/worker4.log 2>&1 &","P5=$!",
                "python -m gensim.models.lsi_worker > {0}Logs/worker5.log 2>&1 &","P6=$!",
                "python -m gensim.models.lsi_dispatcher > {0}Logs/dispatcher.log 2>&1 &","P7=$!"],
            "body": [
                "kmer_lsi.py -i {0}hashed_reads/ -o {0}cluster_vectors/","kill $P1 $P2 $P3 $P4 $P5 $P6 $P7"]},

        "KmerClusterIndex": {
            "outfile": "KmerClusterIndex_Job.q",
            "header": [
                "#SBATCH -J KmerClusterIndex",
                "#SBATCH -o {0}Logs/KmerClusterIndex-Out.out",
                "#SBATCH -e {0}Logs/KmerClusterIndex-Err.err",
                "#SBATCH -q week",
                "#SBATCH -R 'rusage[mem=1]'",
                "#SBATCH -M 35"],
            "body": [
                "kmer_cluster_index.py -i {0}hashed_reads/ -o {0}cluster_vectors/ -t 0.7",
                "create_jobs.py -j KmerClusterParts -i ./","X=`sed -n 1p hashed_reads/hashParts.txt`",
                "sed -i 's/%parts%/$X/g' LSFScripts/KmerClusterParts_ArrayJob.q","create_jobs.py -j LSFScripts/KmerClusterMerge -i ./","X=`sed -n 1p cluster_vectors/numClusters.txt`",
                "sed -i 's/%clusters%/$X/g' LSFScripts/KmerClusterMerge_ArrayJob.q"]},

        "KmerClusterParts": {
            "outfile": "KmerClusterParts_ArrayJob.q",
            "array": ["hashed_reads/","*.hashq.*"],
            "header": [
                "#SBATCH -J KmerClusterParts[1-%parts%]",
                "#SBATCH -o {0}Logs/KmerClusterParts-Out-%I.out",
                "#SBATCH -e {0}Logs/KmerClusterParts-Err-%I.err",
                "#SBATCH -q hour",
                "#SBATCH -t 3:59",
                "#SBATCH -R 'rusage[mem=1:argon_io=3]'",
                "#SBATCH -M 4"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "kmer_cluster_part.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}cluster_vectors/ -t 0.7"]},

        "KmerClusterMerge": {
            "outfile": "KmerClusterMerge_ArrayJob.q",
            "array": ["hashed_reads/","*.hashq.*"],
            "header": [
                "#SBATCH -J KmerClusterMerge[1-%clusters%]",
                "#SBATCH -o {0}Logs/KmerClusterMerge-Out-%I.out",
                "#SBATCH -e {0}Logs/KmerClusterMerge-Err-%I.err",
                "#SBATCH -q hour",
                "#SBATCH -t 3:59",
                "#SBATCH -R 'rusage[mem=1]'",
                "#SBATCH -M 8"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "kmer_cluster_merge.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}cluster_vectors/ -o {0}cluster_vectors/"]},

        "KmerClusterCols": {
            "outfile": "KmerClusterCols_Job.q",
            "header": [
                "#SBATCH -J KmerClusterCols",
                "#SBATCH -o {0}Logs/KmerClusterCols-Out.out",
                "#SBATCH -e {0}Logs/KmerClusterCols-Err.err",
                "#SBATCH -q flower",
                "#SBATCH -t 71:58",
                "#SBATCH -R 'rusage[mem=40]'",
                "#SBATCH -M 70"],
            "body": [
                "kmer_cluster_cols.py -i {0}hashed_reads/ -o {0}cluster_vectors/"]},

        "ReadPartitions": {
            "outfile": "ReadPartitions_ArrayJob.q",
            "array": ["hashed_reads/","*.hashq.*"],
            "header": [
                "#SBATCH -J ReadPartitions[1-",
                "#SBATCH -o {0}Logs/ReadPartitions-Out-%I.out",
                "#SBATCH -e {0}Logs/ReadPartitions-Err-%I.err",
                "#SBATCH -q week",
                "#SBATCH -t 45:10",
                "#SBATCH -R 'rusage[mem=3:argon_io=3]'",
                "#SBATCH -M 20"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "write_partition_parts.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}cluster_vectors/ -t TMPDIR"]},

        "MergeIntermediatePartitions": {
            "outfile": "MergeIntermediatePartitions_ArrayJob.q",
            "array": ["cluster_vectors/","*.cluster.npy"],
            "header": [
                "#SBATCH -J MergeIntermediatePartitions[1-",
                "#SBATCH -o {0}Logs/MergeIntermediatePartitions-Out-%I.out",
                "#SBATCH -e {0}Logs/MergeIntermediatePartitions-Err-%I.err",
                "#SBATCH -q hour",
                "#SBATCH -t 1:55",
                "#SBATCH -M 2",
                "#SBATCH -R 'rusage[argon_io=3]'"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "merge_partition_parts.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}cluster_vectors/ -o {0}read_partitions/"]},

        "SplitPairs": {
            "outfile": "SplitPairs_ArrayJob.q",
            "array": ["cluster_vectors/","*.cluster.npy"],
            "header": [
                "#SBATCH -J SplitPairs[1-",
                "#SBATCH -o {0}Logs/SplitPairs-Out-%I.out",
                "#SBATCH -e {0}Logs/SplitPairs-Err-%I.err",
                "#SBATCH -q hour",
                "#SBATCH -t 3:59",
                "#SBATCH -R 'rusage[argon_io=3]'",
                "#SBATCH -M 8"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "split_read_pairs.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}read_partitions/ -o {0}read_partitions/"]},

        "PhylerClassify": {
            "outfile": "PhylerClassify_ArrayJob.q",
            "array": ["cluster_vectors/","*.cluster.npy"],
            "header": [
                "#SBATCH -J PhylerClassify[1-",
                "#SBATCH -o {0}Logs/PhylerClassify-Out-%I.out",
                "#SBATCH -e {0}Logs/PhylerClassify-Err-%I.err",
                "#SBATCH -q hour",
                "#SBATCH -t 3:55",
                "#SBATCH -M 4","source /broad/software/scripts/useuse",
                "reuse BLAST"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "phyler_classify.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}read_partitions/ -o {0}phyler/"]},

        "PhylerIdentify": {
            "outfile": "PhylerIdentify_ArrayJob.q",
            "array": ["cluster_vectors/","*.cluster.npy"],
            "header": [
                "#SBATCH -J PhylerIdentify[1-",
                "#SBATCH -o {0}Logs/PhylerIdentify-Out-%I.out",
                "#SBATCH -e {0}Logs/PhylerIdentify-Err-%I.err",
                "#SBATCH -q hour",
                "#SBATCH -t 3:55",
                "#SBATCH -M 2"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "phyler_identify.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}read_partitions/ -o {0}phyler/"]},

        "PhylerSummary": {
            "outfile": "PhylerSummary_Job.q",
            "header": [
                "#SBATCH -J PhylerSummary",
                "#SBATCH -o {0}Logs/PhylerSummary-Out.out",
                "#SBATCH -e {0}Logs/PhylerSummary-Err.err",
                "#SBATCH -q hour",
                "#SBATCH -t 1:55",
                "#SBATCH -M 2"],
            "body": [
                "phyler_summary.py -i {0}phyler/"]}
    },

    "CommonElements": {
        "header": ["#!/bin/bash"],
        "body": ["echo Date: `date`","t1=`date +%s`"],
        "footer": ["[ $? -eq 0 ] || echo 'JOB FAILURE: $?'","echo Date: `date`","t2=`date +%s`","tdiff=`echo 'scale=3;('$t2'-'$t1')/3600' | bc`","echo 'Total time:  '$tdiff' hours'"]
    }
}