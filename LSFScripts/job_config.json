{ 
    "JobParams": {

        "CreateHash": {
            "outfile": "CreateHash_Job.q",
            "header": [
                "#SBATCH -J CreateHash",
                "#SBATCH -p 1week",
                "#SBATCH -o {0}Logs/CreateHash.out",
                "#SBATCH -e {0}Logs/CreateHash.err",
                "#SBATCH -t 2-00:00:00",
                "module purge",
                "module load Biopython"],
            "body": [
                "create_hash.py -i {0}original_reads/ -o {0}hashed_reads/ -k 33 -s 31"]},

        "HashReads": {
            "outfile": "HashReads_ArrayJob.q",
            "array": ["original_reads/","*.fastq.*"],
            "header": [
                "#SBATCH -J HashReads[1-{array_size}]",
                "#SBATCH --array=1-{array_size}",
                "#SBATCH -p 1day",
                "#SBATCH -o {0}Logs/HashReads%a.out",
                "#SBATCH -e {0}Logs/HashReads%a.err",
                "#SBATCH -t 02:00:00",
                "module purge",
                "module load Biopython"],
            "body": [
                "hash_fastq_reads.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}original_reads/ -o {0}hashed_reads/"]},

        "MergeHash": {
            "outfile": "MergeHash_ArrayJob.q",
            "array": ["original_reads/","*.fastq",5],
            "header": [
                "#SBATCH -J MergeHash[1-{array_size}]",
                "#SBATCH --array=1-{array_size}",
                "#SBATCH -p 1day",
                "#SBATCH -o {0}Logs/MergeHash-%a.out",
                "#SBATCH -e {0}Logs/MergeHash-%a.err",
                "#SBATCH --mem-per-cpu=20G",
                "#SBATCH -t 01:00:00",
                "module purge",
                "module load Biopython"],
            "body": [
                "merge_hashq_files.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}hashed_reads/"]},

        "CombineFractions": {
            "outfile": "CombineFractions_ArrayJob.q",
            "array": ["original_reads/","*.fastq",1],
            "header": [
                "#SBATCH -J CombineFractions[1-{array_size}]",
                "#SBATCH --array=1-{array_size}",
                "#SBATCH -p htc",
                "#SBATCH -o {0}Logs/CombineFractions-%a.out",
                "#SBATCH -e {0}Logs/CombineFractions-%a.err",
                "#SBATCH --mem-per-cpu=20G",
                "#SBATCH -t 30:00",
                "module purge",
                "module load Biopython"],
            "body": [
                "merge_hashq_fractions.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}hashed_reads/"]},

        "GlobalWeights": {
            "outfile": "GlobalWeights_Job.q",
            "header": [
                "#SBATCH -J GlobalWeights",
                "#SBATCH -p 1day",
                "#SBATCH -o {0}Logs/GlobalWeights.out",
                "#SBATCH -e {0}Logs/GlobalWeights.err",
                "#SBATCH -t 2:00:00"],
            "body": [
                "tfidf_corpus.py -i {0}hashed_reads/ -o {0}cluster_vectors/"]},

        "KmerCorpus": {
            "outfile": "KmerCorpus_ArrayJob.q",
            "array": ["hashed_reads/","*.count.hash"],
            "header": [
                "#SBATCH -J KmerCorpus[1-{array_size}]",
                "#SBATCH --array=1-{array_size}",
                "#SBATCH -p htc",
                "#SBATCH -o {0}Logs/KmerCorpus-%a.out",
                "#SBATCH -e {0}Logs/KmerCorpus-%a.err",
                "#SBATCH -t 5:00"],
            "body": [
                "kmer_corpus.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}cluster_vectors/"]},

        "KmerLSI": {
            "outfile": "KmerLSI_Job.q",
            "header": [
                "#SBATCH -J KmerLSI",
                "#SBATCH -p 1week",
                "#SBATCH -o {0}Logs/KmerLSI.out",
                "#SBATCH -e {0}Logs/KmerLSI.err",
                "#SBATCH -n 6",
                "#SBATCH -R 'rusage[mem=4] span[hosts=1]'",
                "#SBATCH -M 10",
                "python -m Pyro4.naming -n 0.0.0.0 > {0}Logs/nameserver.log 2>&1 &","P1=$!","python -m gensim.models.lsi_worker > {0}Logs/worker1.log 2>&1 &","P2=$!",
                "python -m gensim.models.lsi_worker > {0}Logs/worker2.log 2>&1 &","P3=$!",
                "python -m gensim.models.lsi_worker > {0}Logs/worker3.log 2>&1 &","P4=$!",
                "python -m gensim.models.lsi_worker > {0}Logs/worker4.log 2>&1 &","P5=$!",
                "python -m gensim.models.lsi_worker > {0}Logs/worker5.log 2>&1 &","P6=$!",
                "python -m gensim.models.lsi_dispatcher > {0}Logs/dispatcher.log 2>&1 &","P7=$!"],
            "body": [
                "kmer_lsi.py -i {0}hashed_reads/ -o {0}cluster_vectors/","kill $P1 $P2 $P3 $P4 $P5 $P6 $P7"]},

        "KmerClusterIndex": {
            "outfile": "KmerClusterIndex_Job.q",
            "header": [
                "#SBATCH -J KmerClusterIndex",
                "#SBATCH -p 1week",
                "#SBATCH -o {0}Logs/KmerClusterIndex.out",
                "#SBATCH -e {0}Logs/KmerClusterIndex.err",
                "#SBATCH -R 'rusage[mem=1]'",
                "#SBATCH -M 35"],
            "body": [
                "kmer_cluster_index.py -i {0}hashed_reads/ -o {0}cluster_vectors/ -t 0.7",
                "create_jobs.py -j KmerClusterParts -i ./","X=`sed -n 1p hashed_reads/hashParts.txt`",
                "sed -i 's/%parts%/$X/g' LSFScripts/KmerClusterParts_ArrayJob.q","create_jobs.py -j LSFScripts/KmerClusterMerge -i ./","X=`sed -n 1p cluster_vectors/numClusters.txt`",
                "sed -i 's/%clusters%/$X/g' LSFScripts/KmerClusterMerge_ArrayJob.q"]},

        "KmerClusterParts": {
            "outfile": "KmerClusterParts_ArrayJob.q",
            "array": ["hashed_reads/","*.hashq.*"],
            "header": [
                "#SBATCH -J KmerClusterParts[1-%parts%]",
                "#SBATCH -p htc",
                "#SBATCH -o {0}Logs/KmerClusterParts-%a.out",
                "#SBATCH -e {0}Logs/KmerClusterParts-%a.err",
                "#SBATCH -R 'rusage[mem=1:argon_io=3]'",
                "#SBATCH -M 4"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "kmer_cluster_part.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}cluster_vectors/ -t 0.7"]},

        "KmerClusterMerge": {
            "outfile": "KmerClusterMerge_ArrayJob.q",
            "array": ["hashed_reads/","*.hashq.*"],
            "header": [
                "#SBATCH -J KmerClusterMerge[1-%clusters%]",
                "#SBATCH -p htc",
                "#SBATCH -o {0}Logs/KmerClusterMerge-%a.out",
                "#SBATCH -e {0}Logs/KmerClusterMerge-%a.err",
                "#SBATCH -R 'rusage[mem=1]'",
                "#SBATCH -M 8"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "kmer_cluster_merge.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}cluster_vectors/ -o {0}cluster_vectors/"]},

        "KmerClusterCols": {
            "outfile": "KmerClusterCols_Job.q",
            "header": [
                "#SBATCH -J KmerClusterCols",
                "#SBATCH -p 1day",
                "#SBATCH -o {0}Logs/KmerClusterCols.out",
                "#SBATCH -e {0}Logs/KmerClusterCols.err",
                "#SBATCH -R 'rusage[mem=40]'",
                "#SBATCH -M 70"],
            "body": [
                "kmer_cluster_cols.py -i {0}hashed_reads/ -o {0}cluster_vectors/"]},

        "ReadPartitions": {
            "outfile": "ReadPartitions_ArrayJob.q",
            "array": ["hashed_reads/","*.hashq.*"],
            "header": [
                "#SBATCH -J ReadPartitions[1-",
                "#SBATCH -p 1week",
                "#SBATCH -o {0}Logs/ReadPartitions-%a.out",
                "#SBATCH -e {0}Logs/ReadPartitions-%a.err",
                "#SBATCH -R 'rusage[mem=3:argon_io=3]'",
                "#SBATCH -M 20"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "write_partition_parts.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}hashed_reads/ -o {0}cluster_vectors/ -t TMPDIR"]},

        "MergeIntermediatePartitions": {
            "outfile": "MergeIntermediatePartitions_ArrayJob.q",
            "array": ["cluster_vectors/","*.cluster.npy"],
            "header": [
                "#SBATCH -J MergeIntermediatePartitions[1-",
                "#SBATCH -p htc",
                "#SBATCH -o {0}Logs/MergeIntermediatePartitions-%a.out",
                "#SBATCH -e {0}Logs/MergeIntermediatePartitions-%a.err",
                "#SBATCH -M 2",
                "#SBATCH -R 'rusage[argon_io=3]'"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "merge_partition_parts.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}cluster_vectors/ -o {0}read_partitions/"]},

        "SplitPairs": {
            "outfile": "SplitPairs_ArrayJob.q",
            "array": ["cluster_vectors/","*.cluster.npy"],
            "header": [
                "#SBATCH -J SplitPairs[1-",
                "#SBATCH -p htc",
                "#SBATCH -o {0}Logs/SplitPairs-%a.out",
                "#SBATCH -e {0}Logs/SplitPairs-%a.err",
                "#SBATCH -R 'rusage[argon_io=3]'",
                "#SBATCH -M 8"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "split_read_pairs.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}read_partitions/ -o {0}read_partitions/"]},

        "PhylerClassify": {
            "outfile": "PhylerClassify_ArrayJob.q",
            "array": ["cluster_vectors/","*.cluster.npy"],
            "header": [
                "#SBATCH -J PhylerClassify[1-",
                "#SBATCH -p htc",
                "#SBATCH -o {0}Logs/PhylerClassify-%a.out",
                "#SBATCH -e {0}Logs/PhylerClassify-%a.err",
                "#SBATCH -M 4","source /broad/software/scripts/useuse",
                "reuse BLAST"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "phyler_classify.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}read_partitions/ -o {0}phyler/"]},

        "PhylerIdentify": {
            "outfile": "PhylerIdentify_ArrayJob.q",
            "array": ["cluster_vectors/","*.cluster.npy"],
            "header": [
                "#SBATCH -J PhylerIdentify[1-",
                "#SBATCH -p htc",
                "#SBATCH -o {0}Logs/PhylerIdentify-%a.out",
                "#SBATCH -e {0}Logs/PhylerIdentify-%a.err",
                "#SBATCH -M 2"],
            "body": [
                "sleep $(($SLURM_ARRAY_TASK_ID % 60))",
                "phyler_identify.py -r ${{SLURM_ARRAY_TASK_ID}} -i {0}read_partitions/ -o {0}phyler/"]},

        "PhylerSummary": {
            "outfile": "PhylerSummary_Job.q",
            "header": [
                "#SBATCH -J PhylerSummary",
                "#SBATCH -p htc",
                "#SBATCH -o {0}Logs/PhylerSummary.out",
                "#SBATCH -e {0}Logs/PhylerSummary.err",
                "#SBATCH -M 2"],
            "body": [
                "phyler_summary.py -i {0}phyler/"]}
    },

    "CommonElements": {
        "header": ["#!/bin/bash"],
        "body": ["echo Date: `date`","t1=`date +%s`"],
        "footer": ["[ $? -eq 0 ] || echo 'JOB FAILURE: $?'","echo Date: `date`","t2=`date +%s`","tdiff=`echo 'scale=3;('$t2'-'$t1')/3600' | bc`","echo 'Total time:  '$tdiff' hours'"]
    }
}